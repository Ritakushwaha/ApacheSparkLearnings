{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2c04de3",
   "metadata": {},
   "source": [
    "# Writing Windows Function\n",
    "\n",
    "You need to write Spark code to perform the steps mentioned below. You\n",
    "should write it to optimize for execution speed.\n",
    "\n",
    "#### 1. Filter the complete dataset for DISH = (Biryani or Pizza or Dosa) from X, where X is the complete dataset\n",
    "#### 2. Group by ORDERID, STOREID, PRICE\n",
    "#### 3. If the number of records in the group are > 1î‚’\n",
    "#####         3.1 Calculate no. of records per dish\n",
    "#####         3.2 Calculate total no.of records\n",
    "######       In case you can't find such a record, drop all these records\n",
    "##### 3.3 Update the column JUSTANOTHERFEATURE with a list of distinct values which are greater than 1 in that column for the group.\n",
    "#### 4. Else do nothing.\n",
    "#### 5. Merge it back with X and write to the output to a S3 path partitioned by the DISH name.\n",
    "\n",
    "In case you assume anything about the problem statement while writing the code for it, please do mention it.\n",
    "\n",
    "\n",
    "### We can get Dataset from the given link\n",
    "https://drive.google.com/file/d/1JbO0JS9UKZdqhwLrTRVYfivTLycfOrpC/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9fc6cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rita/Documents/Spark/Assignments\r\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env bash\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea89ee9e",
   "metadata": {},
   "source": [
    "## Important points:\n",
    "\n",
    "#### 1. Python : 3.8.10\n",
    "#### 2. Spark : 3.2.0\n",
    "#### 3. Hadoop : 3.3.1\n",
    "\n",
    "### I have added additional Libraries to  - SPARK_HOME/jars \n",
    "\n",
    "#### 1. aws-java-sdk-bundle-1.12.151.jar\n",
    "#### 2. hadoop-aws-3.3.1.jar \n",
    "#### 3. jets3t-0.9.4.jar\n",
    "\n",
    "###### sudo wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.1/hadoop-aws-3.3.1.jar \n",
    "###### sudo wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.151/aws-java-sdk-bundle-1.12.151.jar \n",
    "###### sudo wget https://repo1.maven.org/maven2/net/java/dev/jets3t/jets3t/0.9.4/jets3t-0.9.4.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40d8083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, window,substring, col,col, udf, collect_list, collect_set, count, when,sum\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import  *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07807343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "tree = ET.parse(r'/home/rita/Desktop/aws_keys/aws_keys.xml'.format(os.path))\n",
    "\n",
    "def getData(Data):\n",
    "    root = tree.getroot()\n",
    "    for ele in root.findall('add'):\n",
    "        key = ele.get('key')\n",
    "        value = ele.get('value')\n",
    "        if(Data == key):\n",
    "            result = value\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0f8fec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key = getData(\"access_key\")\n",
    "secret_key = getData(\"secret_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ca6dbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/04 20:01:30 WARN Utils: Your hostname, EMPID21092 resolves to a loopback address: 127.0.1.1; using 192.168.1.6 instead (on interface wlp3s0)\n",
      "22/02/04 20:01:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/04 20:01:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/02/04 20:01:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/02/04 20:01:32 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    ".appName(\"Window Function Assignment\") \\\n",
    ".config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    ".config(\"spark.hadoop.fs.s3a.fast.upload\",\"true\") \\\n",
    ".config(\"spark.hadoop.fs.s3a.multiobjectdelete.enable\", \"false\") \\\n",
    ".config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.WebIdentityTokenCredentialsProvider\") \\\n",
    ".master(\"local[*]\").getOrCreate()\n",
    "data = spark.read.format(\"parquet\")\\\n",
    ".options(header='true', inferschema='true', delimiter=',')\\\n",
    ".load(\"/home/rita/Documents/Spark/Assignments/spark_assignment_data.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac7d277",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data.toPandas().to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e7a9d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1129292, 7)\n"
     ]
    }
   ],
   "source": [
    "print((data.count(), len(data.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4022670",
   "metadata": {},
   "source": [
    "### Filter the complete dataset for DISH = (Biryani or Pizza or Dosa) from X, where X is the complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f01280",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dishes = ['Biryani','Pizza','Dosa']\n",
    "filtered_df = data.filter(data.DISH.isin(dishes))\n",
    "#filtered_df.toPandas().to_csv('filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8d0686a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(805280, 7)\n"
     ]
    }
   ],
   "source": [
    "print((filtered_df.count(), len(filtered_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d46eb03f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|   DISH|count(DISH)|\n",
      "+-------+-----------+\n",
      "|   Dosa|       4938|\n",
      "|Biryani|        585|\n",
      "|  Pizza|     799757|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dish_part = Window.partitionBy(\"DISH\")\n",
    "\n",
    "count_dishes = filtered_df.select('DISH').groupby(col('DISH')).agg(count(col('DISH')))\n",
    "count_dishes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d229628",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "partition_ = Window.partitionBy(\"ORDERID\",\"STOREID\",\"PRICE\").orderBy(\"DISH\")\n",
    "\n",
    "grouped_df = filtered_df.withColumn(\"records\", count(\"*\").over(partition_))\n",
    "#grouped_df.toPandas().to_csv('grouped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e93b3b2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|   DISH|count(DISH)|\n",
      "+-------+-----------+\n",
      "|   Dosa|       4938|\n",
      "|Biryani|        585|\n",
      "|  Pizza|     799757|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_dishes = grouped_df.select('DISH').groupby(col('DISH')).agg(count(col('DISH')))\n",
    "count_dishes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c9bb66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_greater_than_1 = grouped_df.filter(grouped_df.records > 1)\n",
    "#records_greater_than_1.toPandas().to_csv('records_greater_than_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd562102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "| DISH|count(DISH)|\n",
      "+-----+-----------+\n",
      "| Dosa|        403|\n",
      "|Pizza|      13229|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "count_dishes = records_greater_than_1.select('DISH').groupby(col('DISH')).agg(count(col('DISH')))\n",
    "count_dishes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3be134a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 19:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   13632|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "no_of_records = records_greater_than_1.agg(count(\"*\"))\n",
    "no_of_records.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d598a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "#spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "#spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "#spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70f4ec01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/04 20:07:30 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    records_greater_than_1.coalesce(1) \\\n",
    "    .write.option(\"header\",True) \\\n",
    "        .option(\"maxRecordsPerFile\", 5000) \\\n",
    "        .partitionBy(\"DISH\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"parquet\") \\\n",
    "        .save(\"s3a://giruu/output_\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ca7447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
