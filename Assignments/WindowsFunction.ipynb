{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2c04de3",
   "metadata": {},
   "source": [
    "# Writing Windows Function\n",
    "\n",
    "You need to write Spark code to perform the steps mentioned below. You\n",
    "should write it to optimize for execution speed.\n",
    "\n",
    "#### 1. Filter the complete dataset for DISH = (Biryani or Pizza or Dosa) from X, where X is the complete dataset\n",
    "#### 2. Group by ORDERID, STOREID, PRICE\n",
    "#### 3. If the number of records in the group are > 1î‚’\n",
    "#####         3.1 Calculate no. of records per dish\n",
    "#####         3.2 Calculate total no.of records\n",
    "######       In case you can't find such a record, drop all these records\n",
    "##### 3.3 Update the column JUSTANOTHERFEATURE with a list of distinct values which are greater than 1 in that column for the group.\n",
    "#### 4. Else do nothing.\n",
    "#### 5. Merge it back with X and write to the output to a S3 path partitioned by the DISH name.\n",
    "\n",
    "In case you assume anything about the problem statement while writing the code for it, please do mention it.\n",
    "\n",
    "\n",
    "### We can get Dataset from the given link\n",
    "https://drive.google.com/file/d/1JbO0JS9UKZdqhwLrTRVYfivTLycfOrpC/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e9fc6cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rita/Documents/Spark/Assignments\r\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env bash\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40d8083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, window,substring, col,col, udf, collect_list, collect_set, count, when,sum\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import  *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ca6dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Window Function Assignment\").master(\"local[*]\").getOrCreate()\n",
    "data = spark.read.format(\"parquet\")\\\n",
    ".options(header='true', inferschema='true', delimiter=',')\\\n",
    ".load(\"/home/rita/Documents/Spark/Assignments/spark_assignment_data.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ac7d277",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data.toPandas().to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e7a9d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1129292, 7)\n"
     ]
    }
   ],
   "source": [
    "print((data.count(), len(data.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4022670",
   "metadata": {},
   "source": [
    "### Filter the complete dataset for DISH = (Biryani or Pizza or Dosa) from X, where X is the complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36f01280",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dishes = ['Biryani','Pizza','Dosa']\n",
    "filtered_df = data.filter(data.DISH.isin(dishes))\n",
    "#filtered_df.toPandas().to_csv('filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8d0686a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(805280, 7)\n"
     ]
    }
   ],
   "source": [
    "print((filtered_df.count(), len(filtered_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d46eb03f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|   DISH|count(DISH)|\n",
      "+-------+-----------+\n",
      "|   Dosa|       4938|\n",
      "|Biryani|        585|\n",
      "|  Pizza|     799757|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dish_part = Window.partitionBy(\"DISH\")\n",
    "\n",
    "count_dishes = filtered_df.select('DISH').groupby(col('DISH')).agg(count(col('DISH')))\n",
    "count_dishes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d229628",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "partition_ = Window.partitionBy(\"ORDERID\",\"STOREID\",\"PRICE\").orderBy(\"DISH\")\n",
    "\n",
    "grouped_df = filtered_df.withColumn(\"records\", count(\"*\").over(partition_))\n",
    "#grouped_df.toPandas().to_csv('grouped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e93b3b2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|   DISH|count(DISH)|\n",
      "+-------+-----------+\n",
      "|   Dosa|       4938|\n",
      "|Biryani|        585|\n",
      "|  Pizza|     799757|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_dishes = grouped_df.select('DISH').groupby(col('DISH')).agg(count(col('DISH')))\n",
    "count_dishes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c9bb66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_greater_than_1 = grouped_df.filter(grouped_df.records > 1)\n",
    "#records_greater_than_1.toPandas().to_csv('records_greater_than_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd562102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "| DISH|count(DISH)|\n",
      "+-----+-----------+\n",
      "| Dosa|        403|\n",
      "|Pizza|      13229|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_dishes = records_greater_than_1.select('DISH').groupby(col('DISH')).agg(count(col('DISH')))\n",
    "count_dishes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3be134a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   13632|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_of_records = records_greater_than_1.agg(count(\"*\"))\n",
    "no_of_records.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "087ffa5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsudo wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.11.30/aws-java-sdk-1.11.30.jar\\nsudo wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar\\nsudo wget https://repo1.maven.org/maven2/net/java/dev/jets3t/jets3t/0.9.4/jets3t-0.9.4.jar\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "sudo wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.11.30/aws-java-sdk-1.11.30.jar\n",
    "sudo wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar\n",
    "sudo wget https://repo1.maven.org/maven2/net/java/dev/jets3t/jets3t/0.9.4/jets3t-0.9.4.jar\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d598a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"AKIATCEGQATGUTELSLUZ\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"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\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\",\"org.apache.hadoop.fs.s3a.BasicAWSCredentialsProvider\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbf91bb1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'try:\\n    records_greater_than_1.coalesce(1)     .write.option(\"header\",True)         .option(\"maxRecordsPerFile\", 5000)         .partitionBy(\"DISH\")         .mode(\"overwrite\")         .format(\"json\")     .save(\"output_\")\\nexcept Exception as e:\\n    print(e)'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''try:\n",
    "    records_greater_than_1.coalesce(1) \\\n",
    "    .write.option(\"header\",True) \\\n",
    "        .option(\"maxRecordsPerFile\", 5000) \\\n",
    "        .partitionBy(\"DISH\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"json\") \\\n",
    "    .save(\"output_\")\n",
    "except Exception as e:\n",
    "    print(e)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70f4ec01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o179.save.\n",
      ": java.lang.NoClassDefFoundError: com/amazonaws/services/s3/model/MultiObjectDeleteException\n",
      "\tat java.base/java.lang.Class.forName0(Native Method)\n",
      "\tat java.base/java.lang.Class.forName(Class.java:398)\n",
      "\tat org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2604)\n",
      "\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2569)\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2665)\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:461)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:556)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.ClassNotFoundException: com.amazonaws.services.s3.model.MultiObjectDeleteException\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n",
      "\t... 29 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    records_greater_than_1.coalesce(1) \\\n",
    "    .write.option(\"header\",True) \\\n",
    "        .option(\"maxRecordsPerFile\", 5000) \\\n",
    "        .partitionBy(\"DISH\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"json\") \\\n",
    "    .save(\"s3a://giruu/output_\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ca7447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
